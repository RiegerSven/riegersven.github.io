---
title: Introduction to Multilevel Modeling
author: "Sven Rieger"
email: "sven.rieger@uni-tuebingen.de"
email-obfuscation: javascript
date: 12/01/23
date-format: long
published-title: "Last updated on"
abstract: "This material introduces the logic of multilevel modeling and relies (largely) on the book *Hierarchical Linear Models: Applications and Data Analysis Methods* [@raudenbush2002]. The focus is on (simple) two-level models (i.e., no three-level, or cross-classified models) with continuous outcome variables (i.e., no binary or ordered-categorical outcomes)."
format:
  html: default
  revealjs:         
    output-file: intro-mlm-reveal.html
    footer: "This is a test"
    self-contained: true
    smaller: true
    scrollable: true
    navigation-mode: linear
    progress: true
    keyboard: true
    controls: true
    controls-tutorial: true
    controls-layout: edges
    mouse-wheel: false
    slide-number: c/t
    show-slide-number: all
    reference-location: document
    theme: [simple, "../revealjs-styles.scss"]
---

::: {.content-hidden when-format="revealjs"}

## Revealjs Presentation

If you want to see the presentation in full screen go to **Other Formats** on the right.

<div class="container">
  <iframe class="responsive-iframe" src="intro-mlm-reveal.html"></iframe>
</div> 

:::

## Preface: Software I

- The following packages are used:

```{r}
#| label: mlm-pkg
#| echo: true
#| code-fold: false
#| output-location: column
mlmPkg <- c("merTools",
            "lme4",
            "flextable",
            "psych",
            "ggplot2")
```

::: {.fragment}

- Install packages when not already installed:

```{r}
#| label: mlmPkg-install
#| echo: true
#| code-fold: false
#| results: hide
lapply(mlmPkg,
        function(x) 
          if(!x %in% rownames(installed.packages())) {
            install.packages(x)
            }
      )

```

:::

::: {.fragment}

- Load (a subset of) the required package(s) into the `R` session.

```{r}
#| label: load-mlmPkg
#| echo: true
#| code-fold: false
#| message: false
#| output-location: column
library(lme4)
library(ggplot2)
library(flextable)
```

:::

## Preface: Software II

Print list of packages and cite them via [Pandoc citation](<https://pandoc.org/MANUAL.html#citations>){target="_blank"}.

```{r}
#| label: write-mlmPkg
#| code-fold: true
#| code-summary: "Show/hide fenced code"
#| echo: fenced
#| output-location: fragment
#| output: asis

for (i in 1:length(mlmPkg)) {
  
  cat(paste0(i, ". ",
             mlmPkg[i],
             " [", "v", utils::packageVersion(mlmPkg[i]),", @R-", mlmPkg[i],
             "]\n"))
}
    

```


## Preface: Dataset

We use the HSB dataset from the `merTools` package [@R-merTools]:

```{r}
#| label: mlm-dat
#| echo: true
#| code-line-numbers: false
#| code-fold: false
#| eval: true
#| output-location: column-fragment
dat <- merTools::hsb
head(dat, 6)
```

## Why multilevel modelling?

- Environment (often) shows natural hierarchical structures (e.g., students nested within classes/schools, time points within persons)

- The hierarchical structures may (!) lead to dependency in data
  - ignoring the hierarchical data structure leads to underestimation of standard errors and biased test statistics/p-values
  - violation of the independent and identically distributed residuals (i.d.d) assumption

- Hierarchical structures as a research object/topic
  - examining relations on the different level (e.g., [BFLPE](https://en.wikipedia.org/wiki/Big-fish%E2%80%93little-pond_effect){target="_blank"} or [ecological fallacy](https://en.wikipedia.org/wiki/Ecological_fallacy){target="_blank"})
  - effects/relations vary between clusters

::: aside
for a graphical introduction see <http://mfviz.com/hierarchical-models/>
:::

## Preliminaries: Logic of hierarchical linear models I

**Example:** The relation between socio-economic status and achievement across all (actually a subsample) schools

::: {.panel-tabset}

### Figure

```{r}
#| label: plot-ses-math
#| echo: true
#| code-fold: true
#| output-location: default
#| fig-align: center
#| out-width: 80%

dat |>
  (\(d) d[1:550,])() |>
  ggplot(aes(x = ses, y = mathach)) + 
    geom_point(shape=21,
              fill="white",
               color="black",
               size=2) + 
    geom_smooth(method='lm',
                formula=y~x,
                color = "blue",
                se = F,
                linewidth = 1.5) +
    geom_smooth(method='loess',
                formula=y~x,
                color = "red",
                se = F,
                linewidth = 1.5) +
    labs(x = "SES",
         y = "Math Achievemenet") +
    theme_classic() 

```

### Model

```{r}
#| label: fit-lin-mod
#| code-fold: true
#| output-location: default
dat |>
  (\(d) d[1:550,])() |>
  lm(formula = mathach ~ ses) |>
  summary()

```

:::

## Preliminaries: Logic of hierarchical linear models II

**Example:** The relation between socio-economic status and achievement in **two** selected schools

```{r}
#| label: plot-ses-math-2schools
#| echo: true
#| code-fold: true
#| fig-align: center
#| out-width: 70%

dat |>
  subset(subset = schid == 1224 | schid == 1462) |>
  ggplot(aes(x = ses, y = mathach)) +
    geom_point(shape=21,
               fill="white",
               color="black",
               size=2) +
    geom_smooth(method='lm',
               formula=y~x, color = "blue", se = F) +
    geom_smooth(method='loess',
               formula=y~x, color = "red", se = F) +
    facet_wrap(~schid, nrow = 1) +
    labs(x = "SES",
         y = "Math Achievemenet") +
    theme_classic() 
```


:::: {.columns }
::: {.column width="49%"}

$ùê∏(Y‚îÇX)= 10.81 + 2.51X$

:::
::: {.column width="2%"}

:::
::: {.column width="49%"}

$E(Y‚îÇX)=9.994‚àí0.82X$

:::
::::

## Preliminaries: Logic of hierarchical linear models III

**Example:** The relation between socio-economic status and achievement separated in *J* (14) schools

```{r}
#| label: plot-ses-math-14schools
#| echo: true
#| code-fold: true
#| fig-align: center
#| out-width: 70%
ggplot(dat[1:550,],
       aes(x = ses, y = mathach)) + 
  geom_point(shape=21,
             fill="white",
             color="black",
             size=2) +
  geom_smooth(method='lm',
              formula=y~x, 
              se = F, 
              color = "blue") +
  geom_smooth(method='loess',
              formula=y~x, 
              se = F,
              color = "red") +
  facet_wrap(~schid, nrow = 2) +
  labs(x = "SES",
       y = "Math Achievemenet") +
  theme_classic()
```

## Excursion: Fixed effects approach I

- One way to address the hierarchical data structure is to include indicator variables (i.e., dummy variables) for the clusters in the model &rarr; **fixed effects approach**

- Using reference coding we build $(k-1)$ indicator variables $I_c$

- Model equation (see @eq-fixed-eff):
 
$$
Y = \beta_0 + \beta_1X + \beta_2I_2 + \beta_3I_3 + \dots + \beta_kI_k
$${#eq-fixed-eff}


## Excursion: Fixed effects approach II

- **Example:** The relation between socio-economic status $(X)$ and achievement $(Y)$ in *J* (160!) schools

- Model equation (see @eq-fixed-eff-examp):

$$
Y = \beta_0 + \beta_1X + \sum\limits_{2}^{160} \beta_kI_k
$${#eq-fixed-eff-examp}

- The regression has already 162 parameters!
  - Assumption:  same relation between $Y$ and $X$ within all schools
  - ‚ÄúSolution‚Äù: to include 159 interaction terms between SES and indicator variables

- "Problem": Overfitting & number of parameters &rarr; **random effects approach** (see next [slides](#rand-eff))

## Random effects approach {#rand-eff}

Instead of estimating a huge number of regression coefficients, it is possible to estimate only the distribution parameters
(i.e., expected value and variance) of these parameters across clusters

In doing so, there are additional assumptions:

- Level 1 residuals within all clusters follow a (multivariate) normal distribution (when the outcome is continuous):  $r_{ij} \sim N(0,\sigma^2)$

- Level 2 residuals (i.e., random effects) follow a multivariate normal distribution
  - let vector $\mathbf{u} = (u_{0j}, u_{1j}, \dots, u_{kj})$ of level-2 residuals in a two-level model with $k$ predictors $X_{ij1}, X_{ij2}, \dots, X_{ijk}$, then $u \sim N(0,\Sigma_u)$
  - From the example above (i.e., a two-level model with one level-1 predictor ($X_{ij}$) with random effects)

$$
\left( \begin{matrix}u_{0j} \\ u_{1j}\end{matrix}\right) \sim N\left(\begin{bmatrix} 0 \\ 0\end{bmatrix}, \begin{bmatrix} VAR(u_{0j}) & COV(u_{0j}, u_{1j}) \\ COV(u_{1j}, u_{0j}) & VAR(u_{1j})\end{bmatrix}\right)
$$

## Model Equation

::: {.callout-note appearance="simple" icon="false" title="Description of this relationship within any school $j$ by the equations"}

**Level-specific equations:**
$$
\text{Level 1: } Y_{ij} = \beta_{0j} + \beta_{1j}X_{ij} + r_{ij}
$${#eq-general-lvl1}



$$
\begin{aligned}
\text{Level 2 (intercept): }& \beta_{0j} = \gamma_{00} + u_{0j}
\end{aligned}
$${#eq-general-lv2-1}


$$
\begin{aligned}
\text{Level 2 (slope): } &\beta_{1j} = \gamma_{01} + u_{1j}
\end{aligned}
$${#eq-general-lv2-2}

**Combined equation:** Substituting @eq-general-lv2-1 and @eq-general-lv2-2 into @eq-general-lvl1 yields the combined model equation (see @eq-general-comb):

$$
Y_{ij} = \gamma_{00} + u_{0j} + (\gamma_{01} + u_{1j})X_{ij} + r_{ij} 
$${#eq-general-comb}

:::

<!--

::: {style="text-align: right; padding-right: 10px;"}
$\text{where}\; r_{ij} \sim N(0,\sigma^2)$
:::

::: {style="text-align: right; padding-right: 10px;"}
$\text{where}\; \mathbf{u} = (u_{0j}, u_{1j}) \sim N(0,\Sigma_u)$
:::

-->


## Meaning of the parameters

- Random intercept: $\beta_{0j} = \gamma_{00} + u_{0j}$
  - $E(\beta_{0j}) = \gamma_{00}$ &rarr; average cluster intercept across the cluster
  - $VAR(\beta_{0j}) = \tau_{00}$ &rarr; variance of cluster intercepts
  
- Random slope: $\beta_{1j} = \gamma_{01} + u_{1j}$
  - $E(\beta_{1j}) = \gamma_{01}$ &rarr; average slope across the cluster
  - $VAR(\beta_{1j}) = \tau_{11}$ &rarr; variance of slopes
  
- Covariance between slopes and intercepts: $COV(\beta_{0j}, \beta_{1j}) = \tau_{01}$

- Fixed effects: $\gamma_{00}, \gamma_{01}$

- Random effects/residuals: $u_{0j}, u_{1j}, r_{ij}$


## A closer look at the random effects and residuals I

**Random effects/ level 2 residuals:**

- Intercept 

$$
\begin{aligned} 
& \beta_{0j} = \gamma_{00} + u_{0j} \text{(rearrange equation)} \\
& $u_{0j} = \beta_{0j} - \gamma_{00}
\end{aligned} 
$$  

&rarr; deviation of the cluster specific intercept ($\beta_{0j}$) from the average intercept across cluster ($\gamma_{00}$)

- Slope

$\beta_{1j} = \gamma_{01} + u_{1j}$ (rearrange equation)

$u_{1j} = \beta_{1j} - \gamma_{01}$  &rarr; deviation of the cluster specific slope ($\beta_{1j}$) from the average slope across cluster ($\gamma_{01}$)

## A closer look at the random effects and residuals II

**Random effects/ level 1 residual:**

Recall EQ\@ref(eq:comb-eq):

$Y_{ij} = \gamma_{00} + u_{0j} + (\gamma_{01} + u_{1j})X_{ij} + r_{ij}$ (rearrange equation)

$r_{ij} = Y_{ij} - \gamma_{00} - u_{0j} - \gamma_{01}X_{ij} - u_{1j}X_{ij}$ &rarr; deviation of the observed value ($Y_{ij}$) from the conditional expected value, given the predictor ($X_{ij}$) in cluster $j$

<!--

## Graphical visualization

```{r}
#| label: display-mlm
#| echo: false
#| eval: false
p <- ggplot(data = data.frame(x = c(0,0)), mapping = aes(x = x)) +
  geom_point(aes(x=1, y=4), colour="black", size = 2) + # yit
  annotate(geom="text", x=1.2, y=4, label=expression("Y"["ij"]),
           color="black", size = 5) + 
  scale_y_continuous(limits = c(0,6), breaks=seq(0, 6, by = 1)) +
  scale_x_continuous(limits = c(-1,6), breaks=seq(-1, 6, by = 1)) +
  stat_function( fun = function(x) {0.5 + .6*x},
                 geom="line",
                 aes(color = "cluster-specific slope"), size = 2) +
  annotate(geom="text", x=0.2, y=.35, label=expression(beta["0j"]),
           color="black", size = 5) +
  stat_function( fun = function(x) {2 + .25*x},
                 geom="line",
                 aes(color = "average slope averaged across the cluster"), size = 2) +
  annotate(geom="text", x=0.2, y=1.75, label=expression(gamma["00"]),
           color="black", size = 5) +
  geom_vline(xintercept = 0, linetype = "dotted") +
  geom_segment(aes(x = 0, y = .5, xend = 0, yend = 2),
               color = "red", linetype = "dashed") + # rit
  annotate(geom="text", x=1.2, y=3, label=expression("r"["ij"]),
           color="red", size = 5) + 
  geom_segment(aes(x = 1, y = 4, xend = 1, yend = 1.1),
               color = "red", linetype = "dashed") + # u0j
  annotate(geom="text", x=-0.2, y=1, label=expression("u"["0j"]),
           color="red", size = 5) + 
  geom_segment(aes(x = 4.285712, y = 3.0714, xend = 4.285712+1, yend = 3.0714), #gamma01
               color = "black", linetype = "dashed") +
  geom_segment(aes(x = 4.285712+1, y = 3.0714, xend = 4.285712+1, yend = 3.321428), #gamma01
               color = "black", linetype = "dashed") +
  annotate(geom="text", x=4.285712+1.2, y=3.15, label=expression(gamma["01"]),
           color="black", size = 5) + 
  geom_segment(aes(x = 4.285712, y = 3.0714, xend = 4.285712, yend = 3.671427), #beta1j
               color = "black", linetype = "dashed") +
  geom_segment(aes(x = 4.285712, y = 3.671427, xend = 4.285712+1, yend = 3.671427), #beta1j
               color = "black", linetype = "dashed") +
  annotate(geom="text", x=4.1, y=3.4, label=expression(beta["1j"]),
           color="black", size = 5) + 
  geom_segment(aes(x = 4.285712+1, y = 3.321428, xend = 4.285712+1, yend = 3.671427), #beta1j
               color = "red", linetype = "dashed") +
  annotate(geom="text", x=4.285712+1.2, y=3.55, label=expression("u"["0j"]),
           color="red", size = 5) + 
  scale_colour_manual(values = c("black", "blue")) +
  jtools::theme_apa() + theme(legend.position="bottom",
                              text = element_text(size=35),
                              axis.title.x = element_text(size = 35),
                              axis.title.y = element_text(size = 35))
p

```

-->

## Overview of Multilevel Models

- [One-Way ANOVA with Random Effects](random-anova.qmd)
- Model with level-2 predictors: Means-as-Outcomes Regression
- Random-ANCOVA (only Random-Intercept Model)
- Random-Coefficient Modell (Random-Intercept + Random-Slope Model)
- Model with level-2 predictors + cross-level interaction
- ... there are more!

## References {#mlm-intro-ref}

::: {#refs}
:::